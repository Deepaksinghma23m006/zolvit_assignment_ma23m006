{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9634584,"sourceType":"datasetVersion","datasetId":5882354},{"sourceId":9634803,"sourceType":"datasetVersion","datasetId":5882514},{"sourceId":9647744,"sourceType":"datasetVersion","datasetId":5892285},{"sourceId":9668827,"sourceType":"datasetVersion","datasetId":5908275},{"sourceId":9674879,"sourceType":"datasetVersion","datasetId":5912836},{"sourceId":186059600,"sourceType":"kernelVersion"},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-20T11:47:06.173238Z","iopub.execute_input":"2024-10-20T11:47:06.173649Z","iopub.status.idle":"2024-10-20T11:47:06.653157Z","shell.execute_reply.started":"2024-10-20T11:47:06.173601Z","shell.execute_reply":"2024-10-20T11:47:06.652131Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.safetensors.index.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/LICENSE\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/USE_POLICY.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer_config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_text_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/test_tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/requirements.txt\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/eval_details.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/special_tokens_map.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/__init__.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_chat_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/setup.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation_config.json\n/kaggle/input/test-data/INV-145_Indraja Mohite.pdf\n/kaggle/input/test-data/INV-142_Urmila Jangam.pdf\n/kaggle/input/test-data/INV-123_Asit.pdf\n/kaggle/input/test-data/INV-128_Atia Latif.pdf\n/kaggle/input/test-data/INV-144_Atia Latif.pdf\n/kaggle/input/test-data/INV-143_Prashant.pdf\n/kaggle/input/test-data/INV-121_Jitesh Soni.pdf\n/kaggle/input/test-data/INV-138_Agrani Kandele.pdf\n/kaggle/input/test-data/INV-147_Divya Suhane.pdf\n/kaggle/input/test-data/INV-149_Karishma Bande.pdf\n/kaggle/input/test-data/INV-133_Sheetal Kapur.pdf\n/kaggle/input/test-data/INV-124_Ankita Sattva.pdf\n/kaggle/input/test-data/INV-134_Sheetal Kapur.pdf\n/kaggle/input/test-data/INV-148_harshit rathore.pdf\n/kaggle/input/test-data/INV-150_Bhusan Naresh.pdf\n/kaggle/input/test-data/INV-146_Abhikaran Jalonha.pdf\n/kaggle/input/test-data/INV-127_Avik Mallick.pdf\n/kaggle/input/test-data/INV-135_Mohith Saragur.pdf\n/kaggle/input/test-data/INV-136_Rishabh Ramola.pdf\n/kaggle/input/test-data/INV-129_Divya Suhane.pdf\n/kaggle/input/test-data/INV-117_Naman.pdf\n/kaggle/input/test-data/INV-118_Rashu.pdf\n/kaggle/input/test-data/INV-140_Ankit.pdf\n/kaggle/input/test-data/INV-141_Kasturi Kalwar.pdf\n/kaggle/input/liitle3/INV-114_Vaibhav Bhagat.pdf\n/kaggle/input/liitle3/INV-102_Kasturi Kalwar.pdf\n/kaggle/input/liitle3/INV-135_Mohith Saragur.pdf\n/kaggle/input/smallinvoic/INV-114_Vaibhav Bhagat.pdf\n/kaggle/input/smallinvoic/INV-73_Avik Mallick.pdf\n/kaggle/input/smallinvoic/INV-135_Mohith Saragur.pdf\n/kaggle/input/smallinvoic/INV-99_Indraja.pdf\n/kaggle/input/inference-llama-3-8b/__results__.html\n/kaggle/input/inference-llama-3-8b/submission.csv\n/kaggle/input/inference-llama-3-8b/__notebook__.ipynb\n/kaggle/input/inference-llama-3-8b/__output__.json\n/kaggle/input/inference-llama-3-8b/custom.css\n/kaggle/input/invoice/INV-113_Raghvendra.pdf\n/kaggle/input/invoice/INV-147_Divya Suhane.pdf\n/kaggle/input/invoice/INV-149_Karishma Bande.pdf\n/kaggle/input/invoice/INV-133_Sheetal Kapur.pdf\n/kaggle/input/invoice/INV-134_Sheetal Kapur.pdf\n/kaggle/input/invoice/INV-112_Gauri.pdf\n/kaggle/input/invoice/INV-127_Avik Mallick.pdf\n/kaggle/input/invoice/INV-136_Rishabh Ramola.pdf\n/kaggle/input/invoice/INV-104_Joseph Wincet.pdf\n/kaggle/input/invoice/INV-118_Rashu.pdf\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers bitsandbytes accelerate safetensors\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T11:47:06.654594Z","iopub.execute_input":"2024-10-20T11:47:06.655319Z","iopub.status.idle":"2024-10-20T11:47:18.348285Z","shell.execute_reply.started":"2024-10-20T11:47:06.655275Z","shell.execute_reply":"2024-10-20T11:47:18.347359Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport re\nimport logging\nimport time\nfrom pypdf import PdfReader\nfrom pdf2image import convert_from_bytes\nimport pytesseract\nfrom PIL import Image\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# ===========================\n# 1. Configuration and Setup\n# ===========================\n\n# Configure logging\nlogging.basicConfig(\n    filename='invoice_extraction.log', \n    level=logging.INFO,\n    format='%(asctime)s:%(levelname)s:%(message)s'\n)\n\n# Set up Llama model path and tokenizer/model\nMODEL_PATH = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"  # Adjust to a smaller model if possible\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load tokenizer and model\n# Load tokenizer and model without explicitly moving the model to the device\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    offload_buffers=True  # This allows automatic offloading when memory is constrained\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T15:36:20.435525Z","iopub.execute_input":"2024-10-20T15:36:20.436370Z","iopub.status.idle":"2024-10-20T15:37:44.141812Z","shell.execute_reply.started":"2024-10-20T15:36:20.436331Z","shell.execute_reply":"2024-10-20T15:37:44.140860Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e34955340174293983cb582fa46f545"}},"metadata":{}}]},{"cell_type":"code","source":"\n# ==============================\n# 2. Helper Functions\n# ==============================\n\ndef get_pdf_text(pdf_doc):\n    \"\"\"\n    Extracts text from a PDF file, handling regular and scanned PDFs.\n    \n    Parameters:\n        pdf_doc (UploadedFile): The uploaded PDF file.\n    \n    Returns:\n        str: The extracted text from the PDF.\n    \"\"\"\n    text = \"\"\n    try:\n        pdf_reader = PdfReader(pdf_doc)\n        num_pages = len(pdf_reader.pages)\n        for page_number, page in enumerate(pdf_reader.pages, start=1):\n            extracted_text = page.extract_text()\n            if extracted_text and len(extracted_text.strip()) > 50:  # Threshold for text extraction\n                text += extracted_text + \"\\n\"\n                logging.info(f\"Text extracted from page {page_number} using PdfReader.\")\n            else:\n                # If text extraction is insufficient, use OCR\n                logging.info(f\"Insufficient text on page {page_number}. Applying OCR.\")\n                pdf_doc.seek(0)  # Reset file pointer to read bytes\n                pdf_bytes = pdf_doc.read()\n                images = convert_from_bytes(pdf_bytes, first_page=page_number, last_page=page_number)\n                for image in images:\n                    ocr_text = pytesseract.image_to_string(image, config='--psm 6')  # Assume a single uniform block of text\n                    if ocr_text and len(ocr_text.strip()) > 10:  # Threshold for OCR text\n                        text += ocr_text + \"\\n\"\n                        logging.info(f\"OCR text extracted from page {page_number}.\")\n    except Exception as e:\n        logging.error(f\"Error extracting text from PDF: {e}\")\n        print(f\"Error extracting text from PDF: {e}\")\n    return text\n\ndef call_llama_model(pages_data, max_input_length=1000, timeout=300):\n    \"\"\"\n    Calls the Llama 3 model to extract invoice data in JSON format.\n    \n    Parameters:\n        pages_data (str): The extracted text from the PDF.\n        max_input_length (int): Maximum number of tokens to send to the model.\n        timeout (int): Maximum time (in seconds) to wait for model inference.\n    \n    Returns:\n        str or None: The raw extracted data from the model if successful; otherwise, None.\n    \"\"\"\n    prompt_template = '''Extract the following fields from the invoice data: \n- Invoice No.\n- Date\n- Amount\n- Total\n- Email\n- Place of Origin\n- Taxable Value\n- SGST Amount\n- CGST Amount\n- IGST Amount\n- SGST Rate\n- CGST Rate\n- IGST Rate\n- Tax Amount\n- Tax Rate\n- Final Amount\n- Invoice Date\n- Place of Supply\n- GSTIN Supplier\n\nProvide the output strictly in valid JSON format with no additional text, explanations, or comments. \nEnsure all keys are correctly spelled and correspond to the field names above. \nDo not include any trailing commas or syntax errors.\n\nHere is the invoice data:\n{pages}\n'''\n    # Limit the input text\n    pages_data = pages_data[:max_input_length]\n    prompt = prompt_template.format(pages=pages_data)\n\n    try:\n        print(\"Tokenizing input...\")\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_length).to(model.device)\n        print(f\"Input shape: {inputs.input_ids.shape}\")\n\n        print(\"Starting model inference...\")\n        start_time = time.time()\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs, \n                max_new_tokens=500, \n                temperature=0.3,\n                do_sample=True,\n                num_return_sequences=1,\n                eos_token_id=tokenizer.eos_token_id,\n                pad_token_id=tokenizer.eos_token_id,\n            )\n        inference_time = time.time() - start_time\n        print(f\"Model inference completed in {inference_time:.2f} seconds\")\n\n        llm_extracted_data = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"Extracted data length: {len(llm_extracted_data)}\")\n        logging.info(f\"Raw model response for data extraction: {llm_extracted_data}\")\n        return llm_extracted_data\n    except Exception as e:\n        logging.error(f\"Exception during model call: {e}\")\n        print(f\"An error occurred during model call: {e}\")\n        return None\n\ndef validate_data(field, value):\n    \"\"\"\n    Validates the extracted data fields using regex patterns and assigns confidence levels.\n    \n    Parameters:\n        field (str): The name of the field.\n        value (str): The extracted value of the field.\n    \n    Returns:\n        tuple: (is_valid (bool), confidence (str))\n    \"\"\"\n    patterns  = {\n    'Invoice No.': r'^[A-Za-z0-9\\-]+$',                      # Alphanumeric characters and dashes\n    'Date': r'^\\d{2}/\\d{2}/\\d{4}$',                          # Format: DD/MM/YYYY\n    'Amount': r'^\\d+(\\.\\d{1,2})?$',                         # Decimal value (e.g., 100, 100.50)\n    'Total': r'^\\d+(\\.\\d{1,2})?$',                          # Decimal value (e.g., 100, 100.50)\n    'Email': r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$',                   # Valid email format\n    'Place of Origin': r'^[A-Za-z\\s\\-]+$',                   # Alphabetic and spaces\n    'Taxable Value': r'^\\d+(\\.\\d{1,2})?$',                  # Decimal value (e.g., 100, 100.50)\n    'SGST Amount': r'^\\d+(\\.\\d{1,2})?$',                     # Decimal value (e.g., 100, 100.50)\n    'CGST Amount': r'^\\d+(\\.\\d{1,2})?$',                     # Decimal value (e.g., 100, 100.50)\n    'IGST Amount': r'^\\d+(\\.\\d{1,2})?$',                     # Decimal value (e.g., 100, 100.50)\n    'SGST Rate': r'^\\d+(\\.\\d{1,2})?$',                       # Percentage value\n    'CGST Rate': r'^\\d+(\\.\\d{1,2})?$',                       # Percentage value\n    'IGST Rate': r'^\\d+(\\.\\d{1,2})?$',                       # Percentage value\n    'Tax Amount': r'^\\d+(\\.\\d{1,2})?$',                      # Decimal value (e.g., 100, 100.50)\n    'Tax Rate': r'^\\d+(\\.\\d{1,2})?$',                        # Percentage value\n    'Final Amount': r'^\\d+(\\.\\d{1,2})?$',                   # Decimal value (e.g., 100, 100.50)\n    'Invoice Date': r'^\\d{2}/\\d{2}/\\d{4}$',                  # Format: DD/MM/YYYY\n    'Place of Supply': r'^[A-Za-z\\s\\-]+$',                   # Alphabetic and spaces\n    'GSTIN Supplier': r'^\\d{2}[A-Z]{5}\\d{4}[A-Z]{1}[A-Z\\d]{1}[Z]{1}[A-Z\\d]{1}$',  # GSTIN format\n}\n\n\n    if field in patterns:\n        if re.match(patterns[field], str(value).strip()):\n            return True, \"High Confidence\"\n        else:\n            return False, \"Low Confidence\"\n    else:\n        # For fields without specific patterns, basic non-empty check\n        if str(value).strip():\n            return True, \"Medium Confidence\"\n        else:\n            return False, \"Low Confidence\"\n\ndef extract_json(raw_text):\n    \"\"\"\n    Extracts the first JSON object found in the raw text.\n    \n    Parameters:\n        raw_text (str): The raw text containing JSON.\n    \n    Returns:\n        str or None: The extracted JSON string if found; otherwise, None.\n    \"\"\"\n    pattern = r'\\{.*\\}'  # Matches the first occurrence of {...}\n    match = re.search(pattern, raw_text, re.DOTALL)\n    if match:\n        return match.group(0)\n    else:\n        return None\n\n# ===========================================\n# 3. Core Function to Process PDF Files\n# ===========================================\n\ndef create_docs(pdf_file_paths):\n    \"\"\"\n    Processes multiple PDF files to extract invoice data and compile it into a DataFrame.\n    \n    Parameters:\n        pdf_file_paths (list): List of paths to PDF files.\n    \n    Returns:\n        pd.DataFrame: DataFrame containing all extracted invoice data.\n    \"\"\"\n    # Initialize DataFrame with additional columns for confidence and trust\n    df = pd.DataFrame(columns=[\n        'Invoice No.', 'Date', 'Amount', 'Total',\n        'Email', 'Place of Origin', 'Taxable Value', 'SGST Amount',\n        'CGST Amount', 'IGST Amount', 'SGST Rate', 'CGST Rate',\n        'IGST Rate', 'Tax Amount', 'Tax Rate', 'Final Amount',\n        'Invoice Date', 'Place of Supply',\n        'GSTIN Supplier',\n        'Confidence', 'Trust'\n    ])\n    \n    # Initialize a list to hold rows\n    rows = []\n\n    # Metrics tracking\n    metrics = {\n        'total_files': 0,\n        'successful_extractions': 0,\n        'field_accuracy': {field: {'correct': 0, 'total': 0} for field in df.columns if field not in ['Confidence', 'Trust']}\n    }\n    \n    for file_path in pdf_file_paths:\n        metrics['total_files'] += 1\n        file_name = os.path.basename(file_path)\n        print(f\"### Processing {file_name}...\")\n\n        # Read and extract text from PDF\n        pdf_doc = open(file_path, 'rb')\n        pdf_text = get_pdf_text(pdf_doc)\n        pdf_doc.close()\n\n        # Call the Llama model for data extraction\n        llm_extracted_data = call_llama_model(pdf_text)\n        \n        if llm_extracted_data:\n            # Extract JSON from model output\n            json_data = extract_json(llm_extracted_data)\n            if json_data:\n                try:\n                    extracted_fields = json.loads(json_data)\n                    row = {field: extracted_fields.get(field, \"\") for field in df.columns[:-2]}  # All fields except Confidence and Trust\n                    # Validate each field\n                    for field, value in row.items():\n                        is_valid, confidence = validate_data(field, value)\n                        metrics['field_accuracy'][field]['total'] += 1\n                        if is_valid:\n                            metrics['field_accuracy'][field]['correct'] += 1\n                    \n                    row['Confidence'] = confidence\n                    row['Trust'] = \"High\" if all([validate_data(f, row[f])[0] for f in row]) else \"Low\"\n                    \n                    # Add the row to the list\n                    rows.append(row)\n                    metrics['successful_extractions'] += 1\n                except json.JSONDecodeError as e:\n                    logging.error(f\"JSON decode error for {file_name}: {e}\")\n                    print(f\"JSON decode error for {file_name}: {e}\")\n            else:\n                logging.warning(f\"No valid JSON found in model output for {file_name}.\")\n                print(f\"No valid JSON found in model output for {file_name}.\")\n        else:\n            logging.error(f\"Failed to extract data for {file_name}.\")\n\n    # Convert the list of rows to a DataFrame\n    df = pd.DataFrame(rows)\n\n    # Log metrics\n    logging.info(f\"Processed {metrics['total_files']} files with {metrics['successful_extractions']} successful extractions.\")\n    for field, accuracy in metrics['field_accuracy'].items():\n        logging.info(f\"Field: {field}, Accuracy: {accuracy['correct']}/{accuracy['total']}\")\n\n    return df\n\n# ==============================\n# 4. Running the Invoice Extraction\n# ==============================\n\n# Example: Process files from a given directory\npdf_directory = \"/kaggle/input/test-data\"\npdf_file_paths = [os.path.join(pdf_directory, file) for file in os.listdir(pdf_directory) if file.endswith('.pdf')]\nprint(f\"Found {len(pdf_file_paths)} PDF files. Processing...\")\n\n# Process and extract data into DataFrame\nextracted_data_df = create_docs(pdf_file_paths)\n\n# Optionally, save the extracted DataFrame to CSV\noutput_csv_path = \"extracted_invoice_data.csv\"\nextracted_data_df.to_csv(output_csv_path, index=False)\nprint(f\"Extraction completed. Data saved to {output_csv_path}.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T15:37:44.143846Z","iopub.execute_input":"2024-10-20T15:37:44.144610Z","iopub.status.idle":"2024-10-20T17:39:17.736379Z","shell.execute_reply.started":"2024-10-20T15:37:44.144556Z","shell.execute_reply":"2024-10-20T17:39:17.735450Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found 24 PDF files. Processing...\n### Processing INV-145_Indraja Mohite.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 545])\nStarting model inference...\n","output_type":"stream"},{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Model inference completed in 315.12 seconds\nExtracted data length: 3013\n### Processing INV-142_Urmila Jangam.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 532])\nStarting model inference...\nModel inference completed in 303.44 seconds\nExtracted data length: 2981\n### Processing INV-123_Asit.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 564])\nStarting model inference...\nModel inference completed in 303.27 seconds\nExtracted data length: 2802\nJSON decode error for INV-123_Asit.pdf: Extra data: line 22 column 1 (char 557)\n### Processing INV-128_Atia Latif.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 530])\nStarting model inference...\nModel inference completed in 303.45 seconds\nExtracted data length: 2562\n### Processing INV-144_Atia Latif.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 597])\nStarting model inference...\nModel inference completed in 302.65 seconds\nExtracted data length: 2709\n### Processing INV-143_Prashant.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 565])\nStarting model inference...\nModel inference completed in 303.29 seconds\nExtracted data length: 2677\n### Processing INV-121_Jitesh Soni.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 556])\nStarting model inference...\nModel inference completed in 303.23 seconds\nExtracted data length: 2997\n### Processing INV-138_Agrani Kandele.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 559])\nStarting model inference...\nModel inference completed in 303.40 seconds\nExtracted data length: 2644\n### Processing INV-147_Divya Suhane.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 598])\nStarting model inference...\nModel inference completed in 303.01 seconds\nExtracted data length: 2972\nJSON decode error for INV-147_Divya Suhane.pdf: Extra data: line 22 column 1 (char 635)\n### Processing INV-149_Karishma Bande.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 530])\nStarting model inference...\nModel inference completed in 303.53 seconds\nExtracted data length: 2559\n### Processing INV-133_Sheetal Kapur.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 595])\nStarting model inference...\nModel inference completed in 302.61 seconds\nExtracted data length: 2818\n### Processing INV-124_Ankita Sattva.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 561])\nStarting model inference...\nModel inference completed in 303.34 seconds\nExtracted data length: 2672\n### Processing INV-134_Sheetal Kapur.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 525])\nStarting model inference...\nModel inference completed in 303.77 seconds\nExtracted data length: 2875\n### Processing INV-148_harshit rathore.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 558])\nStarting model inference...\nModel inference completed in 303.60 seconds\nExtracted data length: 2827\n### Processing INV-150_Bhusan Naresh.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 526])\nStarting model inference...\nModel inference completed in 303.72 seconds\nExtracted data length: 3244\n### Processing INV-146_Abhikaran Jalonha.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 578])\nStarting model inference...\nModel inference completed in 303.10 seconds\nExtracted data length: 2555\n### Processing INV-127_Avik Mallick.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 538])\nStarting model inference...\nModel inference completed in 303.41 seconds\nExtracted data length: 3012\n### Processing INV-135_Mohith Saragur.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 568])\nStarting model inference...\nModel inference completed in 303.19 seconds\nExtracted data length: 2990\n### Processing INV-136_Rishabh Ramola.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 532])\nStarting model inference...\nModel inference completed in 303.46 seconds\nExtracted data length: 2849\nNo valid JSON found in model output for INV-136_Rishabh Ramola.pdf.\n### Processing INV-129_Divya Suhane.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 557])\nStarting model inference...\nModel inference completed in 303.02 seconds\nExtracted data length: 3148\n### Processing INV-117_Naman.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 573])\nStarting model inference...\nModel inference completed in 303.25 seconds\nExtracted data length: 2958\n### Processing INV-118_Rashu.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 499])\nStarting model inference...\nModel inference completed in 303.20 seconds\nExtracted data length: 2939\n### Processing INV-140_Ankit.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 558])\nStarting model inference...\nModel inference completed in 303.16 seconds\nExtracted data length: 2996\n### Processing INV-141_Kasturi Kalwar.pdf...\nTokenizing input...\nInput shape: torch.Size([1, 548])\nStarting model inference...\nModel inference completed in 303.48 seconds\nExtracted data length: 2923\nExtraction completed. Data saved to extracted_invoice_data.csv.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}